{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jishwithareddy/glowmodel_classification/blob/main/Copy_of_GLOW_Model_Final2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojn5X4BAJs7D",
        "outputId": "f18dca51-d3c7-4bd3-c9ce-57788d30890f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting normflows\n",
            "  Downloading normflows-1.7.2.tar.gz (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from normflows) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from normflows) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->normflows) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->normflows) (1.3.0)\n",
            "Building wheels for collected packages: normflows\n",
            "  Building wheel for normflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normflows: filename=normflows-1.7.2-py2.py3-none-any.whl size=86917 sha256=29a462772fdd2990490e54aac932b1e0f6d39336247807e873b6c6b4b69ec12d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/a4/89/3e09f53a561355c45eccfebeffc07a0e34d36a3f41e3ef68a3\n",
            "Successfully built normflows\n",
            "Installing collected packages: normflows\n",
            "Successfully installed normflows-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install normflows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import numpy as np\n",
        "import normflows as nf\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "io3MQE2FJ0aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model\n",
        "\n",
        "# Define flows\n",
        "L = 2\n",
        "K = 16\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_shape = (1, 28, 28)  # MNIST images are grayscale\n",
        "# channels = 1  # Only one channel for grayscale images\n",
        "\n",
        "n_dims = np.prod(input_shape)\n",
        "channels = 1\n",
        "hidden_channels = 256\n",
        "split_mode = 'channel'\n",
        "scale = True\n",
        "num_classes = 10  # MNIST has 10 classes (digits 0-9)\n",
        "# Set up flows, distributions and merge operations\n",
        "q0 = []\n",
        "merges = []\n",
        "flows = []\n",
        "for i in range(L):\n",
        "    flows_ = []\n",
        "\n",
        "    for j in range(K):\n",
        "       flows_ += [nf.flows.GlowBlock(channels * 2 ** (L + 1 - i), hidden_channels,\n",
        "                             split_mode=split_mode, scale=scale)]\n",
        "\n",
        "    flows_ += [nf.flows.Squeeze()]\n",
        "    flows += [flows_]\n",
        "    if i > 0:\n",
        "        merges += [nf.flows.Merge()]\n",
        "        latent_shape = (input_shape[0] * 2 ** (L - i), input_shape[1] // 2 ** (L - i),\n",
        "                     input_shape[2] // 2 ** (L - i))\n",
        "    else:\n",
        "        latent_shape = (input_shape[0] * 2 ** (L + 1 - i), input_shape[1] // 2 ** (L - i),\n",
        "                     input_shape[2] // 2 ** (L - i))\n",
        "    q0 += [nf.distributions.ClassCondDiagGaussian(latent_shape, num_classes)]\n",
        "\n",
        "\n",
        "# Construct flow model with the multiscale architecture\n",
        "model = nf.MultiscaleFlow(q0, flows, merges)"
      ],
      "metadata": {
        "id": "Xe-DaQm1J2bT",
        "outputId": "9cbff3e1-0bb9-4093-c030-3971581cddfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:738: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
            "LU, pivots = torch.lu(A, compute_pivots)\n",
            "should be replaced with\n",
            "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
            "and\n",
            "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
            "should be replaced with\n",
            "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1992.)\n",
            "  LU, pivots, infos = torch._lu_with_info(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
      ],
      "metadata": {
        "id": "ePi9zEpFJ5JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set batch size and other parameters\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add any other transformations as needed\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist_dataset = datasets.MNIST('datasets/', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "TMQkYLiSJ7Qy",
        "outputId": "6623d1d8-372d-485a-dec3-39b492386618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 139875354.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/MNIST/raw/train-images-idx3-ubyte.gz to datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 46358857.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/MNIST/raw/train-labels-idx1-ubyte.gz to datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 82350667.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6633192.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom function to filter the dataset\n",
        "def filter_dataset(dataset, target_digit):\n",
        "    filtered_data = [(x, y) for x, y in dataset if y == target_digit]\n",
        "    return filtered_data\n",
        "\n",
        "# We can reuse the filter_dataset function to filter datasets for 0 and 1\n",
        "filtered_dataset_zeros = filter_dataset(mnist_dataset, 0)\n",
        "filtered_dataset_ones = filter_dataset(mnist_dataset, 1)\n",
        "\n",
        "# Create separate loaders for 0 and 1\n",
        "train_loader_zeros = DataLoader(filtered_dataset_zeros, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "train_loader_ones = DataLoader(filtered_dataset_ones, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Create separate GLOW models for 0 and 1\n",
        "model_zeros = nf.MultiscaleFlow(q0, flows, merges).to(device)\n",
        "model_ones = nf.MultiscaleFlow(q0, flows, merges).to(device)\n",
        "\n",
        "optimizer_zeros = torch.optim.Adamax(model_zeros.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "optimizer_ones = torch.optim.Adamax(model_ones.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "-z_VonoOKAUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter=100"
      ],
      "metadata": {
        "id": "1LkwRp-IP_sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_glow(model, loader, optimizer, max_iter):\n",
        "    train_iter = iter(loader)\n",
        "    loss_hist = []\n",
        "    for i in tqdm(range(max_iter)):\n",
        "        try:\n",
        "            x, y = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(loader)\n",
        "            x, y = next(train_iter)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.forward_kld(x.to(device), y.to(device))\n",
        "\n",
        "        # 3. Notify if loss is NaN or Inf\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"Loss has problematic value {loss.item()} at iteration {i}\")\n",
        "            break  # halt training\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_hist.append(loss.detach().cpu().item())  # Ensure the loss is scalar\n",
        "    return loss_hist\n",
        "\n",
        "loss_hist_zeros = train_glow(model_zeros, train_loader_zeros, optimizer_zeros, max_iter)\n",
        "loss_hist_ones = train_glow(model_ones, train_loader_ones, optimizer_ones, max_iter)"
      ],
      "metadata": {
        "id": "FmpRuhsUKCdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(model_zeros))\n"
      ],
      "metadata": {
        "id": "dodyWhLYnylR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_mean(model, loader, num_classes=None):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        latents = []\n",
        "        for x, labels in loader:\n",
        "            x = x.to(device)\n",
        "            # If the model is class-conditional, we need to provide class labels `y`\n",
        "            if num_classes is not None:\n",
        "                # Create a one-hot encoding of the labels if necessary\n",
        "                y = torch.zeros(labels.size(0), num_classes, device=device)\n",
        "                y.scatter_(1, labels.unsqueeze(1).to(device), 1)\n",
        "            else:\n",
        "                y = None  # If not class-conditional, y should be None\n",
        "\n",
        "            z = model(x, y) if num_classes is not None else model(x)\n",
        "            latents.append(z.cpu())  # Move latent vectors to CPU memory\n",
        "        latents = torch.cat(latents, dim=0)\n",
        "        mean_latent = latents.mean(dim=0)\n",
        "\n",
        "    # Print the mean latent vector\n",
        "    print(\"Mean latent vector:\", mean_latent)\n",
        "    return mean_latent\n"
      ],
      "metadata": {
        "id": "A_mlKGy4KEsL"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the MNIST test dataset\n",
        "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_dataset_zeros = filter_dataset(mnist_test, 0)\n",
        "test_dataset_ones = filter_dataset(mnist_test, 1)\n",
        "test_loader_zeros = DataLoader(test_dataset_zeros, batch_size=batch_size, shuffle=True)\n",
        "test_loader_ones = DataLoader(test_dataset_ones, batch_size=batch_size, shuffle=True)\n",
        "print(dir(model_zeros))\n",
        "num_classes = 10  # For MNIST, we have 10 classes\n",
        "# mean_latent_zeros = get_latent_mean(model_zeros, train_loader_zeros, num_classes=num_classes)\n",
        "# mean_latent_ones = get_latent_mean(model_ones, train_loader_ones, num_classes=num_classes)\n",
        "mean_latent_zeros = get_latent_mean(model_zeros, train_loader_zeros, num_classes=num_classes)\n",
        "print(\"Mean latent vector for zeros:\", mean_latent_zeros)\n",
        "\n",
        "mean_latent_ones = get_latent_mean(model_ones, train_loader_ones, num_classes=num_classes)\n",
        "print(\"Mean latent vector for ones:\", mean_latent_ones)\n",
        "\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Your `get_latent_mean` and `filter_dataset` functions should be defined above this snippet\n",
        "# Your `model_zeros` and `model_ones` should be already defined and loaded\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "def classify(model, test_loader, mean_latent_target, other_mean_latent, num_classes=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, labels in test_loader:\n",
        "            x = x.to(device)\n",
        "            labels = labels.to(device)  # Make sure labels are also moved to the device\n",
        "            # If the model is class-conditional, we need to provide class labels `y`\n",
        "            if num_classes is not None:\n",
        "                # Ensure labels are LongTensor for scatter_\n",
        "                labels = labels.long()\n",
        "                # Create a one-hot encoding of the labels if necessary\n",
        "                y = torch.zeros(labels.size(0), num_classes, device=device)\n",
        "                y.scatter_(1, labels.unsqueeze(1), 1)\n",
        "            else:\n",
        "                y = None  # If not class-conditional, y should be None\n",
        "\n",
        "            z = model(x, y) if num_classes is not None else model(x)\n",
        "\n",
        "            # Ensure z has two dimensions, if it's currently 1D it's reshaped to 2D with the second dim being 1\n",
        "            if z.dim() == 1:\n",
        "                z = z.view(-1, 1)\n",
        "\n",
        "            # Move mean latent vectors to device and add a batch dimension so they can be broadcasted\n",
        "            mean_latent_target = mean_latent_target.to(device).unsqueeze(0)  # Broadcast shape: [1, feature_size]\n",
        "            other_mean_latent = other_mean_latent.to(device).unsqueeze(0)    # Broadcast shape: [1, feature_size]\n",
        "\n",
        "            # Now `z`, `mean_latent_target`, and `other_mean_latent` can be broadcast against each other\n",
        "            dist_to_target = ((z - mean_latent_target)**2).sum(dim=1).sqrt()\n",
        "            dist_to_other = ((z - other_mean_latent)**2).sum(dim=1).sqrt()\n",
        "\n",
        "            # Predict target if closer to target mean latent\n",
        "            predictions = (dist_to_target < dist_to_other).long()\n",
        "            correct += predictions.sum().item()\n",
        "            total += predictions.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# Now we call the `classify` function with the `num_classes` argument\n",
        "accuracy_zeros = classify(model_zeros, test_loader_zeros, mean_latent_zeros, mean_latent_ones, num_classes=num_classes)\n",
        "accuracy_ones = classify(model_ones, test_loader_ones, mean_latent_ones, mean_latent_zeros, num_classes=num_classes)\n",
        "\n",
        "print(f'Classification accuracy for 0: {accuracy_zeros * 100:.2f}%')\n",
        "print(f'Classification accuracy for 1: {accuracy_ones * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "J5VVIgpfKGk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfc7b04-e1f0-43a4-cfa3-9a005b545a86"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'class_cond', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'flows', 'forward', 'forward_and_log_det', 'forward_kld', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'inverse_and_log_det', 'ipu', 'load', 'load_state_dict', 'log_prob', 'merges', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_levels', 'parameters', 'q0', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_temperature', 'sample', 'save', 'set_extra_state', 'set_temperature', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'transform', 'type', 'xpu', 'zero_grad']\n",
            "Mean latent vector: tensor(-1626.3828)\n",
            "Mean latent vector for zeros: tensor(-1626.3828)\n",
            "Mean latent vector: tensor(-2228.0508)\n",
            "Mean latent vector for ones: tensor(-2228.0508)\n",
            "Classification accuracy for 0: 325.57%\n",
            "Classification accuracy for 1: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(loss_hist_zeros, label=\"Zeros Loss\")\n",
        "plt.plot(loss_hist_ones, label=\"Ones Loss\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss History')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NyWbzCyKSCqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}